{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGduKHFPjyDd"
   },
   "source": [
    "# Load model\n",
    "\n",
    "We will use quantized model (AWQ) to fit model in free Colab tier. vLLM will speed up inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 516,
     "status": "ok",
     "timestamp": 1710190390378,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "ZoCuigrZWrld"
   },
   "outputs": [],
   "source": [
    "# !pip install -q autoawq\n",
    "# !pip install -q vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47255,
     "status": "ok",
     "timestamp": 1710189564714,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "_9fOfxkBELqn",
    "outputId": "0db406dc-d216-4a51-fdbd-64ecbe8bba7f"
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "llm = LLM(model=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\n",
    "          quantization='awq',\n",
    "          dtype='half',\n",
    "          max_model_len=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1710189564715,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "DvHzNfWqG95I",
    "outputId": "03871924-7bb9-44fc-a5e8-100cb49f065d"
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0,\n",
    "                                 max_tokens=8)\n",
    "\n",
    "prompts = [\n",
    "    \"[INST] Return only result witn no explanation: 2 + 2[/INST] = \",\n",
    "    \"[INST] Return only result witn no explanation: 234 * 231 [/INST] = \",\n",
    "]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"\\nPrompt: {prompt!r}, \\nGenerated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710189564715,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "XHPQfjb_Y3br"
   },
   "outputs": [],
   "source": [
    "def clean_output(txt):\n",
    "  x = txt.strip().split()[0].replace(',', '')\n",
    "  try:\n",
    "    x = float(x)\n",
    "    return x\n",
    "  except ValueError:\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710194222843,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "56FjJxooZuF1"
   },
   "outputs": [],
   "source": [
    "PROMPT = \"[INST] Return only result witn no explanation: {inst} [/INST] = \"\n",
    "\n",
    "def calculate(dataset, operation):\n",
    "  p = [PROMPT.format(inst=f'{a} {operation} {b}') for a, b, _ in dataset]\n",
    "  outputs = llm.generate(p, sampling_params)\n",
    "  return [x.outputs[0].text for x in outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "perl5ML9bOLO"
   },
   "source": [
    "# Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1710194207453,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "GkYErzRgbNHx"
   },
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a, b):\n",
    "    return a - b\n",
    "\n",
    "\n",
    "def multiply(a, b):\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def divide(a, b):\n",
    "    if b != 0:\n",
    "        return round(a / b, 2)\n",
    "    else:\n",
    "        return None  # Handle division by zero\n",
    "\n",
    "\n",
    "def generate_dataset(start, end, function):\n",
    "     return [(i, j, function(i, j)) for i in range(start, end + 1) for j in range(start, end + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVPj5Hf7b5ic"
   },
   "source": [
    "## Test adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305282,
     "status": "ok",
     "timestamp": 1710189870797,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "3Eq_7Bg4bNK0",
    "outputId": "66802d5f-efc8-4120-892a-ce98171f676d"
   },
   "outputs": [],
   "source": [
    "add_dataset = generate_dataset(1, 100, add)\n",
    "results_raw = calculate(add_dataset, '+')\n",
    "results = [clean_output(x) for x in results_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1710189870797,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "oSB4wInueivv"
   },
   "outputs": [],
   "source": [
    "y_true = [x[2] for x in add_dataset]\n",
    "y_true = np.array(y_true)\n",
    "results = np.array(results)\n",
    "acc = sum(results == y_true) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1710189870797,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "YbC2m50rlAte",
    "outputId": "09357b77-cad7-4abc-b85d-d3cbd8cd1e10"
   },
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1710189870797,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "PDC7um2rmjmK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_heatmap(dataset, weights):\n",
    "  a_values, b_values, _ = zip(*dataset)\n",
    "\n",
    "  # Creating bins for a and b with bin size 10\n",
    "  a_bins = np.arange(1, 110, 10)\n",
    "  b_bins = np.arange(1, 110, 10)\n",
    "\n",
    "  # Creating a 2D histogram based on the sum of 'True' values\n",
    "  heatmap, xedges, yedges = np.histogram2d(a_values, b_values, bins=[a_bins, b_bins], weights=weights)\n",
    "\n",
    "  # Plotting the heatmap\n",
    "  plt.imshow(heatmap, extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]], origin='lower', cmap='viridis', aspect='auto')\n",
    "  plt.colorbar(label='Sum of True values')\n",
    "  plt.xlabel('a')\n",
    "  plt.ylabel('b')\n",
    "  plt.title('Heatmap with Bin Size 10')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 636,
     "status": "ok",
     "timestamp": 1710189871429,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "G4-aKsnANuDH",
    "outputId": "1768973b-9d06-45de-cad6-7570bfa4c0b2"
   },
   "outputs": [],
   "source": [
    "plot_heatmap(add_dataset, results==y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODJHYiZwQCr8"
   },
   "source": [
    "## Test multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366114,
     "status": "ok",
     "timestamp": 1710190324734,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "9mCoSaeSQCJE",
    "outputId": "0004ffee-023b-4497-9c6a-7c83626526fd"
   },
   "outputs": [],
   "source": [
    "mul_dataset = generate_dataset(1, 100, multiply)\n",
    "results_raw = calculate(mul_dataset, '*')\n",
    "results = [clean_output(x) for x in results_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1710190360576,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "ItrPgE5tRHnL"
   },
   "outputs": [],
   "source": [
    "y_true = [x[2] for x in mul_dataset]\n",
    "y_true = np.array(y_true)\n",
    "results = np.array(results)\n",
    "acc = sum(results == y_true) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710190362316,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "b5m2VWC3RHqy",
    "outputId": "5fd0bd18-16a1-4a4d-fc3d-eb96415b4810"
   },
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 497,
     "status": "ok",
     "timestamp": 1710190365485,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "009-OnnFRHub",
    "outputId": "9b0867f2-9a81-4d5f-a148-4f3070bedc2e"
   },
   "outputs": [],
   "source": [
    "plot_heatmap(mul_dataset, results==y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1KRhLs8aUII"
   },
   "source": [
    "# Finetune\n",
    "\n",
    "Use unsloth, QLoRA and DPO.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !mamba install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=11.8 \\\n",
    "#     -c pytorch -c nvidia -c xformers -c conda-forge -y\n",
    "# !pip install \"unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip uninstall datasets -y\n",
    "# !pip install datasets\n",
    "\n",
    "# import os\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart after this \n",
    "# !pip install bitsandbytes\n",
    "# !pip install xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52749,
     "status": "ok",
     "timestamp": 1710194189691,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "iRmIBOYIc12y"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# import torch\n",
    "# major_version, minor_version = torch.cuda.get_device_capability()\n",
    "# if major_version >= 8:\n",
    "#     # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
    "#     !pip install \"unsloth[colab-ampere] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# else:\n",
    "#     # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
    "#     !pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMrixwH-gftY"
   },
   "outputs": [],
   "source": [
    "# !pip install -q accelerate\n",
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVJV3zhTTlLm"
   },
   "source": [
    "# Finetune dataset\n",
    "\n",
    "Multiplication for  a, b in range 60-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0cH8GVBYhK8"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "train_dataset = generate_dataset(60, 100, multiply)\n",
    "val_dataset = generate_dataset(101, 110, multiply)\n",
    "\n",
    "\n",
    "dpo_dataset_train = {\n",
    "    \"prompt\": [\n",
    "        PROMPT.format(inst=f'{a} * {b}') for a, b, _ in train_dataset\n",
    "    ],\n",
    "    \"chosen\": [\n",
    "        str(x[2]) for x in train_dataset\n",
    "    ],\n",
    "    \"rejected\": [\n",
    "        str(x[2] + random.choice([-5, -4, -3, -2, -1, 1, 2, 3, 4, 5])) for x in train_dataset\n",
    "    ],\n",
    "}\n",
    "\n",
    "dpo_dataset_eval = {\n",
    "    \"prompt\": [\n",
    "        PROMPT.format(inst=f'{a} * {b}') for a, b, _ in val_dataset\n",
    "    ],\n",
    "    \"chosen\": [\n",
    "        str(x[2]) for x in val_dataset\n",
    "    ],\n",
    "    \"rejected\": [\n",
    "        str(x[2] + random.choice([-5, -4, -3, -2, -1, 1, 2, 3, 4, 5])) for x in val_dataset\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dpo_dataset_train = Dataset.from_dict(dpo_dataset_train)\n",
    "dpo_dataset_eval = Dataset.from_dict(dpo_dataset_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjB8DiyjaaGr"
   },
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1710194189692,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "e7g3yKctgPL_"
   },
   "outputs": [],
   "source": [
    "from unsloth import PatchDPOTrainer\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1710194189692,
     "user": {
      "displayName": "Adrianna Tokarska",
      "userId": "09850000505338841002"
     },
     "user_tz": -60
    },
    "id": "y84Fm_Fba2bJ"
   },
   "outputs": [],
   "source": [
    "# ~22min for 3 epochs \n",
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 256\n",
    "\n",
    "# Load model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    ")\n",
    "\n",
    "# Do model patching and add fast LoRA weights\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Dropout = 0 is currently optimized\n",
    "    bias = \"none\",    # Bias = \"none\" is currently optimized\n",
    "    use_gradient_checkpointing = True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(learning_rate=5e-05,\n",
    "                                  num_train_epochs=2,\n",
    "                                  logging_steps=100,\n",
    "                                  per_device_eval_batch_size=8,\n",
    "                                  per_device_train_batch_size=8,\n",
    "                                  warmup_ratio=0.0,\n",
    "                                  output_dir=\"./output\", \n",
    "                                  report_to='none',\n",
    "                                  fp16=not torch.cuda.is_bf16_supported(),\n",
    "                                  bf16=torch.cuda.is_bf16_supported(),)\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    beta=0.1,\n",
    "    train_dataset=dpo_dataset_train,\n",
    "    eval_dataset=dpo_dataset_eval,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdoSFzifa2jx"
   },
   "outputs": [],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle: You can add your huggingface access token, go to: Add-ons -> Secrets \n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# hf_token = user_secrets.get_secret(\"hf_token\")\n",
    "\n",
    "hf_token = \"\"\n",
    "\n",
    "\n",
    "# model.save_pretrained_merged(\"dpo_calc_mistral\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model.push_to_hub_merged(\"adriata/dpo_calc_mistral\", tokenizer, save_method = \"merged_16bit\", token = hf_token)\n",
    "# model.push_to_hub_merged(\"adriata/dpo_calc_mistral\", tokenizer, save_method = \"lora\", token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    PROMPT.format(inst='22 * 3')\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 12, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNTL4I/P7VroscxvksaWAN8",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1UiNMAFd20hStE6bA36auKJC-qGFpEowy",
     "timestamp": 1710174476644
    }
   ]
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Hugging Face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
