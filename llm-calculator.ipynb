{"metadata":{"colab":{"provenance":[{"file_id":"1UiNMAFd20hStE6bA36auKJC-qGFpEowy","timestamp":1710174476644}],"gpuType":"T4","authorship_tag":"ABX9TyNTL4I/P7VroscxvksaWAN8"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load model\n\nWe will use quantized model (AWQ) to fit model in free Colab tier. vLLM will speed up inference.","metadata":{"id":"ZGduKHFPjyDd"}},{"cell_type":"code","source":"!pip install -q autoawq\n!pip install -q vllm","metadata":{"id":"ZoCuigrZWrld","executionInfo":{"status":"ok","timestamp":1710190390378,"user_tz":-60,"elapsed":516,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"execution":{"iopub.status.busy":"2024-03-13T17:41:41.491344Z","iopub.execute_input":"2024-03-13T17:41:41.492244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from vllm import LLM, SamplingParams\nimport torch\nimport numpy as np\n\nllm = LLM(model=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\n          quantization='awq',\n          dtype='half',\n          max_model_len=128)\n","metadata":{"id":"_9fOfxkBELqn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710189564714,"user_tz":-60,"elapsed":47255,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"outputId":"0db406dc-d216-4a51-fdbd-64ecbe8bba7f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampling_params = SamplingParams(temperature=0,\n                                 max_tokens=8)\n\nprompts = [\n    \"[INST] Return only result witn no explanation: 2 + 2[/INST] = \",\n    \"[INST] Return only result witn no explanation: 234 * 231 [/INST] = \",\n]\n\noutputs = llm.generate(prompts, sampling_params)\n\n# Print the outputs.\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"\\nPrompt: {prompt!r}, \\nGenerated text: {generated_text!r}\")","metadata":{"id":"DvHzNfWqG95I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710189564715,"user_tz":-60,"elapsed":9,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"outputId":"03871924-7bb9-44fc-a5e8-100cb49f065d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_output(txt):\n  x = txt.strip().split()[0].replace(',', '')\n  try:\n    x = float(x)\n    return x\n  except ValueError:\n    return","metadata":{"id":"XHPQfjb_Y3br","executionInfo":{"status":"ok","timestamp":1710189564715,"user_tz":-60,"elapsed":3,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PROMPT = \"[INST] Return only result witn no explanation: {inst} [/INST] = \"\n\ndef calculate(dataset, operation):\n  p = [PROMPT.format(inst=f'{a} {operation} {b}') for a, b, _ in dataset]\n  outputs = llm.generate(p, sampling_params)\n  return [x.outputs[0].text for x in outputs]","metadata":{"id":"56FjJxooZuF1","executionInfo":{"status":"ok","timestamp":1710194222843,"user_tz":-60,"elapsed":3,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate dataset","metadata":{"id":"perl5ML9bOLO"}},{"cell_type":"code","source":"def add(a, b):\n    return a + b\n\n\ndef subtract(a, b):\n    return a - b\n\n\ndef multiply(a, b):\n    return a * b\n\n\ndef divide(a, b):\n    if b != 0:\n        return round(a / b, 2)\n    else:\n        return None  # Handle division by zero\n\n\ndef generate_dataset(start, end, function):\n     return [(i, j, function(i, j)) for i in range(start, end + 1) for j in range(start, end + 1)]","metadata":{"id":"GkYErzRgbNHx","executionInfo":{"status":"ok","timestamp":1710194207453,"user_tz":-60,"elapsed":417,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test adding","metadata":{"id":"MVPj5Hf7b5ic"}},{"cell_type":"code","source":"add_dataset = generate_dataset(1, 100, add)\nresults_raw = calculate(add_dataset, '+')\nresults = [clean_output(x) for x in results_raw]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Eq_7Bg4bNK0","executionInfo":{"status":"ok","timestamp":1710189870797,"user_tz":-60,"elapsed":305282,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"outputId":"66802d5f-efc8-4120-892a-ce98171f676d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = [x[2] for x in add_dataset]\ny_true = np.array(y_true)\nresults = np.array(results)\nacc = sum(results == y_true) / len(results)","metadata":{"id":"oSB4wInueivv","executionInfo":{"status":"ok","timestamp":1710189870797,"user_tz":-60,"elapsed":8,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YbC2m50rlAte","executionInfo":{"status":"ok","timestamp":1710189870797,"user_tz":-60,"elapsed":6,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"outputId":"09357b77-cad7-4abc-b85d-d3cbd8cd1e10","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_heatmap(dataset, weights):\n  a_values, b_values, _ = zip(*dataset)\n\n  # Creating bins for a and b with bin size 10\n  a_bins = np.arange(1, 110, 10)\n  b_bins = np.arange(1, 110, 10)\n\n  # Creating a 2D histogram based on the sum of 'True' values\n  heatmap, xedges, yedges = np.histogram2d(a_values, b_values, bins=[a_bins, b_bins], weights=weights)\n\n  # Plotting the heatmap\n  plt.imshow(heatmap, extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]], origin='lower', cmap='viridis', aspect='auto')\n  plt.colorbar(label='Sum of True values')\n  plt.xlabel('a')\n  plt.ylabel('b')\n  plt.title('Heatmap with Bin Size 10')\n\n  plt.show()","metadata":{"id":"PDC7um2rmjmK","executionInfo":{"status":"ok","timestamp":1710189870797,"user_tz":-60,"elapsed":4,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_heatmap(add_dataset, results==y_true)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"G4-aKsnANuDH","executionInfo":{"status":"ok","timestamp":1710189871429,"user_tz":-60,"elapsed":636,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"outputId":"1768973b-9d06-45de-cad6-7570bfa4c0b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test multiplication","metadata":{"id":"ODJHYiZwQCr8"}},{"cell_type":"code","source":"mul_dataset = generate_dataset(1, 100, multiply)\nresults_raw = calculate(mul_dataset, '*')\nresults = [clean_output(x) for x in results_raw]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9mCoSaeSQCJE","executionInfo":{"status":"ok","timestamp":1710190324734,"user_tz":-60,"elapsed":366114,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"outputId":"0004ffee-023b-4497-9c6a-7c83626526fd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = [x[2] for x in mul_dataset]\ny_true = np.array(y_true)\nresults = np.array(results)\nacc = sum(results == y_true) / len(results)","metadata":{"id":"ItrPgE5tRHnL","executionInfo":{"status":"ok","timestamp":1710190360576,"user_tz":-60,"elapsed":616,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5m2VWC3RHqy","executionInfo":{"status":"ok","timestamp":1710190362316,"user_tz":-60,"elapsed":3,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"outputId":"5fd0bd18-16a1-4a4d-fc3d-eb96415b4810","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_heatmap(mul_dataset, results==y_true)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"009-OnnFRHub","executionInfo":{"status":"ok","timestamp":1710190365485,"user_tz":-60,"elapsed":497,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"outputId":"9b0867f2-9a81-4d5f-a148-4f3070bedc2e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finetune\n\nUse unsloth, QLoRA and DPO.  ","metadata":{"id":"j1KRhLs8aUII"}},{"cell_type":"markdown","source":"### Kaggle","metadata":{}},{"cell_type":"code","source":"%%capture\n!mamba install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=11.8 \\\n    -c pytorch -c nvidia -c xformers -c conda-forge -y\n!pip install \"unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git\"\n!pip uninstall datasets -y\n!pip install datasets\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# restart after this \n# !pip install bitsandbytes\n# !pip install xformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Colab","metadata":{}},{"cell_type":"code","source":"%%capture\nimport torch\nmajor_version, minor_version = torch.cuda.get_device_capability()\nif major_version >= 8:\n    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n    !pip install \"unsloth[colab-ampere] @ git+https://github.com/unslothai/unsloth.git\"\nelse:\n    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n    !pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\npass","metadata":{"id":"iRmIBOYIc12y","executionInfo":{"status":"ok","timestamp":1710194189691,"user_tz":-60,"elapsed":52749,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install -q accelerate\n# !pip install bitsandbytes","metadata":{"id":"oMrixwH-gftY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finetune dataset\n\nMultiplication for  a, b in range 60-100.","metadata":{"id":"uVJV3zhTTlLm"}},{"cell_type":"code","source":"import random\n\n\ntrain_dataset = generate_dataset(60, 100, multiply)\nval_dataset = generate_dataset(101, 110, multiply)\n\n\ndpo_dataset_train = {\n    \"prompt\": [\n        PROMPT.format(inst=f'{a} * {b}') for a, b, _ in train_dataset\n    ],\n    \"chosen\": [\n        str(x[2]) for x in train_dataset\n    ],\n    \"rejected\": [\n        str(x[2] + random.choice([-5, -4, -3, -2, -1, 1, 2, 3, 4, 5])) for x in train_dataset\n    ],\n}\n\ndpo_dataset_eval = {\n    \"prompt\": [\n        PROMPT.format(inst=f'{a} * {b}') for a, b, _ in val_dataset\n    ],\n    \"chosen\": [\n        str(x[2]) for x in val_dataset\n    ],\n    \"rejected\": [\n        str(x[2] + random.choice([-5, -4, -3, -2, -1, 1, 2, 3, 4, 5])) for x in val_dataset\n    ],\n}","metadata":{"id":"G0cH8GVBYhK8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ndpo_dataset_train = Dataset.from_dict(dpo_dataset_train)\ndpo_dataset_eval = Dataset.from_dict(dpo_dataset_eval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finetune","metadata":{"id":"FjB8DiyjaaGr"}},{"cell_type":"code","source":"from unsloth import PatchDPOTrainer\nPatchDPOTrainer()","metadata":{"id":"e7g3yKctgPL_","executionInfo":{"status":"aborted","timestamp":1710194189692,"user_tz":-60,"elapsed":5,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ~22min for 3 epochs \nimport torch\nfrom transformers import TrainingArguments\nfrom trl import DPOTrainer\nfrom unsloth import FastLanguageModel\n\nmax_seq_length = 256\n\n# Load model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n)\n\n# Do model patching and add fast LoRA weights\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 64,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 64,\n    lora_dropout = 0, # Dropout = 0 is currently optimized\n    bias = \"none\",    # Bias = \"none\" is currently optimized\n    use_gradient_checkpointing = True,\n)\n\ntraining_args = TrainingArguments(learning_rate=5e-05,\n                                  num_train_epochs=2,\n                                  logging_steps=100,\n                                  per_device_eval_batch_size=8,\n                                  per_device_train_batch_size=8,\n                                  warmup_ratio=0.0,\n                                  output_dir=\"./output\", \n                                  report_to='none',\n                                  fp16=not torch.cuda.is_bf16_supported(),\n                                  bf16=torch.cuda.is_bf16_supported(),)\n\ndpo_trainer = DPOTrainer(\n    model,\n    args=training_args,\n    beta=0.1,\n    train_dataset=dpo_dataset_train,\n    eval_dataset=dpo_dataset_eval,\n    tokenizer=tokenizer,\n)\ndpo_trainer.train()","metadata":{"id":"y84Fm_Fba2bJ","executionInfo":{"status":"aborted","timestamp":1710194189692,"user_tz":-60,"elapsed":4,"user":{"displayName":"Adrianna Tokarska","userId":"09850000505338841002"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args","metadata":{"id":"jdoSFzifa2jx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kaggle: You can add your huggingface access token, go to: Add-ons -> Secrets \n# remove outputs saved by trainer if you will get error \nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_token\")\n\n# model.save_pretrained_merged(\"dpo_calc_mistral\", tokenizer, save_method = \"merged_16bit\",)\n# model.push_to_hub_merged(\"adriata/dpo_calc_mistral\", tokenizer, save_method = \"merged_16bit\", token = hf_token)\nmodel.push_to_hub_merged(\"adriata/dpo_calc_mistral\", tokenizer, save_method = \"lora\", token = hf_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_to_merge = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained(base_model).to(“cuda”), lora_adapter)\n\nmerged_model = model_to_merge.merge_and_unload()\nmerged_model.save_pretrained(merged_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check model ","metadata":{}},{"cell_type":"code","source":"tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = tokenizer(\n[\n    PROMPT.format(inst='22 * 3')\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 12, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}